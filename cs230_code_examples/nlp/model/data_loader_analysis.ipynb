{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.data_loader import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### building kaggle dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially we have only `ner_dataset.csv` in `data/kaggle` and run `python3 build_kaggle_dataset.py`. The structure of files are presented below. We just unrolled sentences and labels and put them into separate files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m.\u001b[00m\r\n",
      "├── \u001b[01;32mner_dataset.csv\u001b[00m\r\n",
      "├── \u001b[01;34mtest\u001b[00m\r\n",
      "│   ├── labels.txt\r\n",
      "│   └── sentences.txt\r\n",
      "├── \u001b[01;34mtrain\u001b[00m\r\n",
      "│   ├── labels.txt\r\n",
      "│   └── sentences.txt\r\n",
      "└── \u001b[01;34mval\u001b[00m\r\n",
      "    ├── labels.txt\r\n",
      "    └── sentences.txt\r\n",
      "\r\n",
      "3 directories, 7 files\r\n"
     ]
    }
   ],
   "source": [
    "!tree ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence #,Word,POS,Tag\r",
      "\r\n",
      "Sentence: 1,Thousands,NNS,O\r",
      "\r\n",
      ",of,IN,O\r",
      "\r\n",
      ",demonstrators,NNS,O\r",
      "\r\n",
      ",have,VBP,O\r",
      "\r\n",
      ",marched,VBN,O\r",
      "\r\n",
      ",through,IN,O\r",
      "\r\n",
      ",London,NNP,B-geo\r",
      "\r\n",
      ",to,TO,O\r",
      "\r\n",
      ",protest,VB,O\r",
      "\r\n",
      ",the,DT,O\r",
      "\r\n",
      ",war,NN,O\r",
      "\r\n",
      ",in,IN,O\r",
      "\r\n",
      ",Iraq,NNP,B-geo\r",
      "\r\n",
      ",and,CC,O\r",
      "\r\n",
      ",demand,VB,O\r",
      "\r\n",
      ",the,DT,O\r",
      "\r\n",
      ",withdrawal,NN,O\r",
      "\r\n",
      ",of,IN,O\r",
      "\r\n",
      ",British,JJ,B-gpe\r",
      "\r\n",
      ",troops,NNS,O\r",
      "\r\n",
      ",from,IN,O\r",
      "\r\n",
      ",that,DT,O\r",
      "\r\n",
      ",country,NN,O\r",
      "\r\n",
      ",.,.,O\r",
      "\r\n",
      "Sentence: 2,Families,NNS,O\r",
      "\r\n",
      ",of,IN,O\r",
      "\r\n",
      ",soldiers,NNS,O\r",
      "\r\n",
      ",killed,VBN,O\r",
      "\r\n",
      ",in,IN,O\r",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!head -30 ner_dataset.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thousands of demonstrators have marched through London to protest the war in Iraq and demand the withdrawal of British troops from that country .\r\n"
     ]
    }
   ],
   "source": [
    "!head -1 train/sentences.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O O O O O O B-geo O O O O O B-geo O O O O O B-gpe O O O O O\r\n"
     ]
    }
   ],
   "source": [
    "!head -1 train/labels.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   33570\r\n"
     ]
    }
   ],
   "source": [
    "! cat train/sentences.txt | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   33570\r\n"
     ]
    }
   ],
   "source": [
    "! cat train/labels.txt | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### building vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now run `python3 build_vocab.py --data_dir data/kaggle` and extract list of tags and list of words. We also store parameters in `dataset_params.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_params.json tags.txt            \u001b[34mtrain\u001b[m\u001b[m               words.txt\r\n",
      "\u001b[31mner_dataset.csv\u001b[m\u001b[m     \u001b[34mtest\u001b[m\u001b[m                \u001b[34mval\u001b[m\u001b[m\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O\r\n",
      "B-geo\r\n",
      "B-gpe\r\n",
      "B-per\r\n",
      "I-geo\r\n",
      "B-org\r\n",
      "I-org\r\n",
      "B-tim\r\n",
      "B-art\r\n",
      "I-art\r\n"
     ]
    }
   ],
   "source": [
    "!head tags.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thousands\r\n",
      "of\r\n",
      "demonstrators\r\n",
      "have\r\n",
      "marched\r\n",
      "through\r\n",
      "London\r\n",
      "to\r\n",
      "protest\r\n",
      "the\r\n"
     ]
    }
   ],
   "source": [
    "!head words.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\r\n",
      "    \"train_size\": 33570,\r\n",
      "    \"dev_size\": 7194,\r\n",
      "    \"test_size\": 7194,\r\n",
      "    \"vocab_size\": 35180,\r\n",
      "    \"number_of_tags\": 17,\r\n",
      "    \"pad_word\": \"<pad>\",\r\n",
      "    \"pad_tag\": \"O\",\r\n",
      "    \"unk_word\": \"UNK\"\r\n",
      "}"
     ]
    }
   ],
   "source": [
    "!cat dataset_params.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   35180\r\n"
     ]
    }
   ],
   "source": [
    "!cat words.txt | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      17\r\n"
     ]
    }
   ],
   "source": [
    "!cat tags.txt | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### constructor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're building standard vocabularies matching words and tags with indicies. We use them for vectorizing sentences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data/kaggle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\r\n",
      "    \"train_size\": 33570,\r\n",
      "    \"dev_size\": 7194,\r\n",
      "    \"test_size\": 7194,\r\n",
      "    \"vocab_size\": 35180,\r\n",
      "    \"number_of_tags\": 17,\r\n",
      "    \"pad_word\": \"<pad>\",\r\n",
      "    \"pad_tag\": \"O\",\r\n",
      "    \"unk_word\": \"UNK\"\r\n",
      "}"
     ]
    }
   ],
   "source": [
    "!cat data/kaggle/dataset_params.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load json file into dict\n",
    "json_path = os.path.join(data_dir, 'dataset_params.json')\n",
    "assert os.path.isfile(json_path), \"No json file found at {}, run build_vocab.py\".format(json_path)\n",
    "dataset_params = utils.Params(json_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_size': 33570,\n",
       " 'dev_size': 7194,\n",
       " 'test_size': 7194,\n",
       " 'vocab_size': 35180,\n",
       " 'number_of_tags': 17,\n",
       " 'pad_word': '<pad>',\n",
       " 'pad_tag': 'O',\n",
       " 'unk_word': 'UNK'}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_params.dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading vocab\n",
    "vocab_path = os.path.join(data_dir, 'words.txt')\n",
    "vocab = {}\n",
    "with open(vocab_path) as f:\n",
    "    for i, l in enumerate(f.read().splitlines()):\n",
    "        vocab[l] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35180"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Thousands', 0),\n",
       " ('of', 1),\n",
       " ('demonstrators', 2),\n",
       " ('have', 3),\n",
       " ('marched', 4),\n",
       " ('through', 5),\n",
       " ('London', 6),\n",
       " ('to', 7),\n",
       " ('protest', 8),\n",
       " ('the', 9)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(vocab.items())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad>\r\n",
      "UNK\r\n"
     ]
    }
   ],
   "source": [
    "# our vocabulary contains <pad> and UNK\n",
    "!tail -2 data/kaggle/words.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35178, 35179)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab['<pad>'], vocab['UNK']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35179, 35178)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# setting the indices for UNKnown words and PADding symbols\n",
    "unk_ind = vocab[dataset_params.unk_word]\n",
    "pad_ind = vocab[dataset_params.pad_word]\n",
    "unk_ind, pad_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'O': 0,\n",
       " 'B-geo': 1,\n",
       " 'B-gpe': 2,\n",
       " 'B-per': 3,\n",
       " 'I-geo': 4,\n",
       " 'B-org': 5,\n",
       " 'I-org': 6,\n",
       " 'B-tim': 7,\n",
       " 'B-art': 8,\n",
       " 'I-art': 9,\n",
       " 'I-per': 10,\n",
       " 'I-gpe': 11,\n",
       " 'I-tim': 12,\n",
       " 'B-nat': 13,\n",
       " 'B-eve': 14,\n",
       " 'I-eve': 15,\n",
       " 'I-nat': 16}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading tags (we require this to map tags to their indices)\n",
    "tags_path = os.path.join(data_dir, 'tags.txt')\n",
    "tag_map = {}\n",
    "with open(tags_path) as f:\n",
    "    for i, t in enumerate(f.read().splitlines()):\n",
    "        tag_map[t] = i\n",
    "tag_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `load_sentences_labels()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's the function that do vectorization that is required for `Embedding` layer: list of lists, where each inner list contains numbers (indicies of words in a the dictionary). So here's the structure of `data` that is returned by `load_data()`:\n",
    "```python\n",
    "data = {'train': {'data': sentences, # list of lists\n",
    "                  'labels': labels,\n",
    "                  'size': len(sentences)}\n",
    "        ...\n",
    "       }\n",
    "```\n",
    "But sentences still have different length. We pad or truncate them later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_file, labels_file = 'data/kaggle/train/sentences.txt', \\\n",
    "                              'data/kaggle/train/labels.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "labels = []\n",
    "with open(sentences_file) as f:\n",
    "    for sentence in f.read().splitlines():\n",
    "        # replace each token by its index if it is in vocab\n",
    "        # else use index of UNK_WORD\n",
    "        s = [vocab[token] if token in vocab \n",
    "             else unk_ind\n",
    "             for token in sentence.split(' ')]\n",
    "        sentences.append(s)\n",
    "\n",
    "with open(labels_file) as f:\n",
    "    for sentence in f.read().splitlines():\n",
    "        # replace each label by its index\n",
    "        l = [tag_map[label] for label in sentence.split(' ')]\n",
    "        labels.append(l)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33570, 33570)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences), dataset_params.dict['train_size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we encode a sentence as a list of indicies in the dictionary\n",
    "sentences[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       " [22, 1, 23, 24, 11, 9, 25, 26, 9, 27],\n",
       " [42, 4, 18, 9, 43, 1, 44, 7, 45, 46],\n",
       " [49, 50, 9, 51, 1, 52, 53, 54, 55, 56],\n",
       " [61, 8, 62, 63, 9, 64, 1, 9, 65, 66]]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[sentences[i][:10] for i in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[24, 30, 14, 15, 25]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sentences are not of the same length, \n",
    "# so we have to pad them\n",
    "[len(sentences[i]) for i in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Thousands',\n",
       " 'of',\n",
       " 'demonstrators',\n",
       " 'have',\n",
       " 'marched',\n",
       " 'through',\n",
       " 'London',\n",
       " 'to',\n",
       " 'protest',\n",
       " 'the']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_vocab = {v: k for k, v in vocab.items()}\n",
    "[inv_vocab[i] for i in sentences[0][:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally we load vectorized data for train/val/test\n",
    "def load_sentences_labels(sentences_file, labels_file, d):\n",
    "    sentences = []\n",
    "    labels = []\n",
    "    with open(sentences_file) as f:\n",
    "        for sentence in f.read().splitlines():\n",
    "            # replace each token by its index if it is in vocab\n",
    "            # else use index of UNK_WORD\n",
    "            s = [vocab[token] if token in vocab \n",
    "                 else unk_ind\n",
    "                 for token in sentence.split(' ')]\n",
    "            sentences.append(s)\n",
    "\n",
    "    with open(labels_file) as f:\n",
    "        for sentence in f.read().splitlines():\n",
    "            # replace each label by its index\n",
    "            l = [tag_map[label] for label in sentence.split(' ')]\n",
    "            labels.append(l)\n",
    "            \n",
    "    d['data'] = sentences\n",
    "    d['labels'] = labels\n",
    "    d['size'] = len(sentences)\n",
    "\n",
    "\n",
    "data = {}\n",
    "types = ['train', 'val']\n",
    "for split in ['train', 'val', 'test']:\n",
    "    if split in types:\n",
    "        sentences_file = os.path.join(data_dir, split, \"sentences.txt\")\n",
    "        labels_file = os.path.join(data_dir, split, \"labels.txt\")\n",
    "        data[split] = {}\n",
    "        load_sentences_labels(sentences_file, labels_file, data[split])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['train', 'val'])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'labels', 'size'])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train']['data'][0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `data_iterator()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function produces batches separately for `train/dev/test` using `batch_size` from parameters file of the model. It also makes padding using the following length:\n",
    "```python\n",
    "batch_max_len = max([len(s) for s in batch_sentences])\n",
    "```\n",
    "It returns generator and we can iterate over it to get `batch_data, batch_labels` on each iteration. Shapes of these values are below. As usual we get: `batch_size, batch_max_len`. These are our setntences padded or truncated.\n",
    "\n",
    "First of all let's run `DataLoader` and then we'll recreate this last function. There's an inconsistency with tag symbol for padding. It's better to store it in a tag_map or somewhere else than just create it in comments in this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data/kaggle'\n",
    "json_path = os.path.join(data_dir, 'dataset_params_ex.json')\n",
    "params = utils.Params(json_path)\n",
    "dl = DataLoader(data_dir, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35180, 17)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dl.vocab), len(dl.tag_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Thousands', 0),\n",
       " ('of', 1),\n",
       " ('demonstrators', 2),\n",
       " ('have', 3),\n",
       " ('marched', 4),\n",
       " ('through', 5),\n",
       " ('London', 6),\n",
       " ('to', 7),\n",
       " ('protest', 8),\n",
       " ('the', 9)]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(dl.vocab.items())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dl.load_data(types=['train', 'val', 'test'], data_dir=data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['train', 'val', 'test'])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'labels', 'size'])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train']['data'][0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally let's create an iterator\n",
    "json_path = os.path.join('experiments/base_model', 'params.json')\n",
    "params = utils.Params(json_path)\n",
    "it = dl.data_iterator(data['train'], params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_data, batch_labels = next(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 30]), torch.Size([5, 30]))"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_data.shape, batch_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    0,     1,     2,     3,     4,     5,     6,     7,     8,     9,\n",
       "           10,    11,    12,    13,    14,     9,    15,     1,    16,    17,\n",
       "           18,    19,    20,    21, 35178, 35178, 35178, 35178, 35178, 35178])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# that's our first sentence padded with <pad>\n",
    "batch_data[0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0,  0,\n",
       "         2,  0,  0,  0,  0,  0, -1, -1, -1, -1, -1, -1])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_labels[0, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try to recreate `data_iterator()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['train', 'val', 'test'])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'labels', 'size'])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.001,\n",
       " 'batch_size': 5,\n",
       " 'num_epochs': 10,\n",
       " 'lstm_hidden_dim': 50,\n",
       " 'embedding_dim': 50,\n",
       " 'save_summary_steps': 100,\n",
       " 'cuda': 0}"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params.dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35178"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6714"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many batches do we have\n",
    "(data['size']+1)//params.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33570"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we remember that 33570 - # of sentences in train data\n",
    "# and batch_size = 5 (see above)\n",
    "data['size'], data['size'] / 5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6714.0"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "33570 / 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create one batch of data. It should be of size `(5, batch_max_len)`. Then we compute `max_len`. To make padding we just create `numpy` arrays with padding symbol and just copy our data in these arrays. And then we just convert `numpy` arrays into `pytorch` tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's take first five sentences\n",
    "order = list(range(data['size']))\n",
    "i = 0\n",
    "batch_sentences = [data['data'][idx] for idx in \n",
    "                   order[i*params.batch_size:(i+1)*params.batch_size]]\n",
    "batch_tags = [data['labels'][idx] for idx in \n",
    "              order[i*params.batch_size:(i+1)*params.batch_size]]\n",
    "len(batch_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['data'][0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_sentences[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[24, 30, 14, 15, 25]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(s) for s in batch_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_max_len = max([len(s) for s in batch_sentences])\n",
    "batch_max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_data = pad_ind*np.ones((len(batch_sentences), batch_max_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 30)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([35178, 35178, 35178, 35178, 35178, 35178, 35178, 35178, 35178,\n",
       "       35178, 35178, 35178, 35178, 35178, 35178, 35178, 35178, 35178,\n",
       "       35178, 35178, 35178, 35178, 35178, 35178, 35178, 35178, 35178,\n",
       "       35178, 35178, 35178])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# it contains padding symbol at all positions\n",
    "batch_data[0].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first sentence is indeed 24 words long\n",
    "# so we expect padding of 6\n",
    "cur_len = len(batch_sentences[0])\n",
    "cur_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_data[0][:cur_len] = batch_sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0,     1,     2,     3,     4,     5,     6,     7,     8,\n",
       "           9,    10,    11,    12,    13,    14,     9,    15,     1,\n",
       "          16,    17,    18,    19,    20,    21, 35178, 35178, 35178,\n",
       "       35178, 35178, 35178])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_data[0].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
