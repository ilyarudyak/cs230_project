{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#v1\n",
    "#26/10/2018\n",
    "\n",
    "\n",
    "dataname=\"epistroma\" #should match the value used to train the network, will be used to load the appropirate model\n",
    "gpuid=0\n",
    "\n",
    "\n",
    "patch_size=256 #should match the value used to train the network\n",
    "batch_size=1 #nicer to have a single batch so that we can iterately view the output, while not consuming too much \n",
    "edge_weight=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_end_of_cell_marker": 2
   },
   "outputs": [],
   "source": [
    "# https://github.com/jvanvugt/pytorch-unet\n",
    "#torch.multiprocessing.set_start_method(\"fork\")\n",
    "import random, sys\n",
    "import cv2\n",
    "import glob\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import scipy.ndimage\n",
    "import skimage\n",
    "import time\n",
    "\n",
    "import tables\n",
    "from skimage import io, morphology\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from unet import UNet\n",
    "\n",
    "import PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.get_device_properties(gpuid))\n",
    "torch.cuda.set_device(gpuid)\n",
    "device = torch.device(f'cuda:{gpuid}' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(f\"{dataname}_unet_best_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the model, note that the paramters are coming from the checkpoint, since the architecture of the model needs to exactly match the weights saved\n",
    "model = UNet(n_classes=checkpoint[\"n_classes\"], in_channels=checkpoint[\"in_channels\"], padding=checkpoint[\"padding\"],depth=checkpoint[\"depth\"],\n",
    "             wf=checkpoint[\"wf\"], up_mode=checkpoint[\"up_mode\"], batch_norm=checkpoint[\"batch_norm\"]).to(device)\n",
    "print(f\"total params: \\t{sum([np.prod(p.size()) for p in model.parameters()])}\")\n",
    "model.load_state_dict(checkpoint[\"model_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this defines our dataset class which will be used by the dataloader\n",
    "class Dataset(object):\n",
    "    def __init__(self, fname ,img_transform=None, mask_transform = None, edge_weight= False):\n",
    "        #nothing special here, just internalizing the constructor parameters\n",
    "        self.fname=fname\n",
    "        self.edge_weight = edge_weight\n",
    "        \n",
    "        self.img_transform=img_transform\n",
    "        self.mask_transform = mask_transform\n",
    "        \n",
    "        self.tables=tables.open_file(self.fname)\n",
    "        self.numpixels=self.tables.root.numpixels[:]\n",
    "        self.nitems=self.tables.root.img.shape[0]\n",
    "        self.tables.close()\n",
    "        \n",
    "        self.img = None\n",
    "        self.mask = None\n",
    "    def __getitem__(self, index):\n",
    "        #opening should be done in __init__ but seems to be\n",
    "        #an issue with multithreading so doing here\n",
    "        if(self.img is None): #open in thread\n",
    "            self.tables=tables.open_file(self.fname)\n",
    "            self.img=self.tables.root.img\n",
    "            self.mask=self.tables.root.mask\n",
    "       \n",
    "        #get the requested image and mask from the pytable\n",
    "        img = self.img[index,:,:,:]\n",
    "        mask = self.mask[index,:,:]\n",
    "        \n",
    "        #the original Unet paper assignes increased weights to the edges of the annotated objects\n",
    "        #their method is more sophistocated, but this one is faster, we simply dilate the mask and \n",
    "        #highlight all the pixels which were \"added\"\n",
    "        if(self.edge_weight):\n",
    "            weight = scipy.ndimage.morphology.binary_dilation(mask==1, iterations =2) & ~mask\n",
    "        else: #otherwise the edge weight is all ones and thus has no affect\n",
    "            weight = np.ones(mask.shape,dtype=mask.dtype)\n",
    "        \n",
    "        mask = mask[:,:,None].repeat(3,axis=2) #in order to use the transformations given by torchvision\n",
    "        weight = weight[:,:,None].repeat(3,axis=2) #inputs need to be 3D, so here we convert from 1d to 3d by repetition\n",
    "        \n",
    "        img_new = img\n",
    "        mask_new = mask\n",
    "        weight_new = weight\n",
    "        \n",
    "        seed = random.randrange(sys.maxsize) #get a random seed so that we can reproducibly do the transofrmations\n",
    "        if self.img_transform is not None:\n",
    "            random.seed(seed) # apply this seed to img transforms\n",
    "            img_new = self.img_transform(img)\n",
    "\n",
    "        if self.mask_transform is not None:\n",
    "            random.seed(seed)\n",
    "            mask_new = self.mask_transform(mask)\n",
    "            mask_new = np.asarray(mask_new)[:,:,0].squeeze()\n",
    "            \n",
    "            random.seed(seed)\n",
    "            weight_new = self.mask_transform(weight)\n",
    "            weight_new = np.asarray(weight_new)[:,:,0].squeeze()\n",
    "\n",
    "        return img_new, mask_new, weight_new\n",
    "    def __len__(self):\n",
    "        return self.nitems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "#note that since we need the transofrmations to be reproducible for both masks and images\n",
    "#we do the spatial transformations first, and afterwards do any color augmentations\n",
    "\n",
    "#in the case of using this for output generation, we want to use the original images since they will give a better sense of the exepected \n",
    "#output when used on the rest of the dataset, as a result, we disable all unnecessary augmentation.\n",
    "#the only component that remains here is the randomcrop, to ensure that regardless of the size of the image\n",
    "#in the database, we extract an appropriately sized patch\n",
    "img_transform = transforms.Compose([\n",
    "     transforms.ToPILImage(),\n",
    "    #transforms.RandomVerticalFlip(),\n",
    "    #transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(size=(patch_size,patch_size),pad_if_needed=True), #these need to be in a reproducible order, first affine transforms and then color\n",
    "    #transforms.RandomResizedCrop(size=patch_size),\n",
    "    #transforms.RandomRotation(180),\n",
    "    #transforms.ColorJitter(brightness=0, contrast=0, saturation=0, hue=.5),\n",
    "    #transforms.RandomGrayscale(),\n",
    "    transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "\n",
    "mask_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    #transforms.RandomVerticalFlip(),\n",
    "    #transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(size=(patch_size,patch_size),pad_if_needed=True), #these need to be in a reproducible order, first affine transforms and then color\n",
    "    #transforms.RandomResizedCrop(size=patch_size,interpolation=PIL.Image.NEAREST),\n",
    "    #transforms.RandomRotation(180),\n",
    "    ])\n",
    "\n",
    "phases=[\"val\"]\n",
    "dataset={}\n",
    "dataLoader={}\n",
    "for phase in phases:\n",
    "    \n",
    "    dataset[phase]=Dataset(f\"./{dataname}_{phase}.pytable\", img_transform=img_transform , mask_transform = mask_transform ,edge_weight=edge_weight)\n",
    "    dataLoader[phase]=DataLoader(dataset[phase], batch_size=batch_size, \n",
    "                                shuffle=True, num_workers=0, pin_memory=True) #,pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "#set the model to evaluation mode, since we're only generating output and not doing any back propogation\n",
    "model.eval()\n",
    "for ii , (X, y, y_weight) in enumerate(dataLoader[\"val\"]):\n",
    "    X = X.to(device)  # [NBATCH, 3, H, W]\n",
    "    y = y.type('torch.LongTensor').to(device)  # [NBATCH, H, W] with class indices (0, 1)\n",
    "\n",
    "    output = model(X)  # [NBATCH, 2, H, W]\n",
    "\n",
    "    output=output.detach().squeeze().cpu().numpy() #get output and pull it to CPU\n",
    "    output=np.moveaxis(output,0,-1)  #reshape moving last dimension\n",
    "    \n",
    "    fig, ax = plt.subplots(1,4, figsize=(10,4))  # 1 row, 2 columns\n",
    "\n",
    "    ax[0].imshow(output[:,:,1])\n",
    "    ax[1].imshow(np.argmax(output,axis=2))\n",
    "    ax[2].imshow(y.detach().squeeze().cpu().numpy())\n",
    "    ax[3].imshow(np.moveaxis(X.detach().squeeze().cpu().numpy(),0,-1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
