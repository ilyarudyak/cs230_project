{
    "learning_rate": 1e-3,
    "batch_size": 32,
    "num_epochs": 10,

    "seq_len": 20,

    "lstm_hidden_dim": 32,
    "embedding_dim": 250,
    "n_layers": 2,
    "dropout": 0.5,

    "save_summary_steps": 100,
    "cuda": 0,

    "vocab_size": 2697,
    "number_of_tags": 2
}
